{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d00529",
   "metadata": {},
   "source": [
    "# AI Offline Parcel Legacy Format\n",
    "\n",
    "This notebook processes pre-downloaded AI feature payloads to produce legacy format parcel rollups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda1244",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import ast\n",
    "import shapely.wkt\n",
    "import geopandas as gpd\n",
    "import concurrent.futures\n",
    "import os\n",
    "\n",
    "from nmaipy.feature_api import FeatureApi\n",
    "from nmaipy import parcels\n",
    "from nmaipy.constants import (\n",
    "    LAT_LONG_CRS,\n",
    "    BUILDING_ID,\n",
    "    ROOF_ID,\n",
    "    TRAMPOLINE_ID,\n",
    "    POOL_ID,\n",
    "    CONSTRUCTION_ID,\n",
    "    SOLAR_ID,\n",
    "    VEG_IDS,\n",
    "    SURFACES_IDS,\n",
    "    ROOF_CHAR_IDS,\n",
    ")\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "# Used for projections\n",
    "COUNTRY = \"us\"\n",
    "# Number of concurrent processes (set to available CPU cores)\n",
    "WORKERS = 4\n",
    "# Batch size used for concurrency\n",
    "BATCH_SIZE = 1e4\n",
    "# Set to limit the number of batches to process, use only for dev testing (set to False to disable)\n",
    "LIMIT = None\n",
    "\n",
    "# Data and output paths\n",
    "BASE_DATA_DIR = Path(\"/home/jovyan/data/parcel_rollup\")\n",
    "SOURCE_DIR = BASE_DATA_DIR / \"source\"\n",
    "BATCHES_DIR = BASE_DATA_DIR / \"batches\"\n",
    "PROCESSED_DIR = BASE_DATA_DIR / \"processed\"\n",
    "OUTPUT_DIR = BASE_DATA_DIR / \"output\"\n",
    "LEGACY_DIR = BASE_DATA_DIR / \"legacy\"\n",
    "\n",
    "BATCHES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LEGACY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Column ordering\n",
    "FIRST_COLUMNS = [\"aoi_id\", \"date\", \"mesh_date\", \"link\", \"system_version\"]\n",
    "LAST_COLUMNS = [\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "079d686d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>description</th>\n",
       "      <th>schema</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0339726f-081e-5a6e-b9a9-42d95c1b5c8a</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Swimming Pool</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3680e1b8-8ae1-5a15-8ec7-820078ef3298</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Solar Panel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753621ee-0b9f-515e-9bcf-ea40b96612ab</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Trampoline</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a2a81381-13c6-57dc-a967-af696e45f6c7</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Construction Site</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a2e4ae39-8a61-5515-9d18-8900aa6e6072</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Building</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c08255a4-ba9f-562b-932c-ff76f2faeeeb</th>\n",
       "      <td>Feature</td>\n",
       "      <td>Roof</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         type        description schema\n",
       "id                                                                     \n",
       "0339726f-081e-5a6e-b9a9-42d95c1b5c8a  Feature      Swimming Pool    NaN\n",
       "3680e1b8-8ae1-5a15-8ec7-820078ef3298  Feature        Solar Panel    NaN\n",
       "753621ee-0b9f-515e-9bcf-ea40b96612ab  Feature         Trampoline    NaN\n",
       "a2a81381-13c6-57dc-a967-af696e45f6c7  Feature  Construction Site    NaN\n",
       "a2e4ae39-8a61-5515-9d18-8900aa6e6072  Feature           Building    NaN\n",
       "c08255a4-ba9f-562b-932c-ff76f2faeeeb  Feature               Roof    NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get class lookup from the AI feature API\n",
    "feature_api = FeatureApi()\n",
    "classes_df = feature_api.get_feature_classes()\n",
    "classes_df = classes_df[classes_df[\"type\"] == \"Feature\"]\n",
    "# Filter to set we're interseted in\n",
    "classes = [BUILDING_ID, ROOF_ID, TRAMPOLINE_ID, POOL_ID, CONSTRUCTION_ID, SOLAR_ID]\n",
    "classes_df = classes_df[classes_df.index.isin(classes)]\n",
    "display(classes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67d9d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch source files into smaller chucks to enable concurrency\n",
    "def batch_csv(source_path):\n",
    "    with open(source_path, \"r\") as fs:\n",
    "        header = fs.readline()\n",
    "        counter = 0\n",
    "        while True:\n",
    "            target_path = BATCHES_DIR / f\"{source_path.stem}_{str(counter).zfill(3)}.csv\"\n",
    "            with open(target_path, \"w\") as ft:\n",
    "                ft.write(header)\n",
    "                for _ in range(int(BATCH_SIZE)):\n",
    "                    line = fs.readline()\n",
    "                    if line == \"\":\n",
    "                        break\n",
    "                    ft.write(line)\n",
    "            counter += 1\n",
    "            if line == \"\" or (LIMIT and counter >= LIMIT):\n",
    "                break\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(WORKERS) as executor:\n",
    "    for source_path in SOURCE_DIR.glob(\"*.csv\"):\n",
    "        executor.submit(batch_csv, source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4004fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(source_path, force=False):\n",
    "    \"\"\"\n",
    "    Process a batch. The output is saved in the AI offline parcel format.\n",
    "    \"\"\"\n",
    "    # Check for output, only run if there is none.\n",
    "    # The temp path is used to make writing output atomic.\n",
    "    outpath = PROCESSED_DIR / source_path.name\n",
    "    outpath_temp = PROCESSED_DIR / f\"{source_path.name}.tmp\"\n",
    "    if outpath.is_file() and not force:\n",
    "        return\n",
    "    \n",
    "    # Read, parse JSON payload to dict and rename parcel ID\n",
    "    source_df = pd.read_csv(source_path, sep=\"|\")\n",
    "    source_df.payload = source_df.payload.apply(ast.literal_eval)\n",
    "    source_df = source_df.rename(columns={\"parcelPtId\": \"aoi_id\"})\n",
    "\n",
    "    # Extract to parcels GeoDataFrame\n",
    "    parcels_gdf = gpd.GeoDataFrame(source_df[['aoi_id']], geometry=source_df.geometry.apply(shapely.wkt.loads))\n",
    "    parcels_gdf = parcels_gdf.set_crs(LAT_LONG_CRS)\n",
    "\n",
    "    # Extract features and metadata GeoDataFrames\n",
    "    payloads = [FeatureApi.payload_gdf(row.payload, row.aoi_id) for row in source_df.itertuples()]\n",
    "    features, metadata = zip(*payloads)\n",
    "    features_gdf = pd.concat(features)\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    \n",
    "    # Filter features based on parcel geometry\n",
    "    features_gdf = parcels.filter_features_in_parcels(\n",
    "        parcels_gdf, features_gdf, country=COUNTRY\n",
    "    )\n",
    "    \n",
    "    # Create rollup\n",
    "    rollup_df = parcels.parcel_rollup(parcels_gdf, features_gdf, classes_df)\n",
    "\n",
    "    # Combine data sets\n",
    "    final_df = rollup_df.merge(parcels_gdf, on=\"aoi_id\")\n",
    "    final_df = final_df.merge(metadata_df, on=\"aoi_id\")\n",
    "    columns = FIRST_COLUMNS + [c for c in final_df.columns if c not in FIRST_COLUMNS + LAST_COLUMNS] + LAST_COLUMNS\n",
    "    final_df = final_df[columns]\n",
    "    final_df = final_df.rename(columns={\"aoi_id\": \"parcelPtId\"})\n",
    "\n",
    "    # Save\n",
    "    final_df.to_csv(outpath_temp, index=False)\n",
    "    os.rename(outpath_temp, outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c492f0b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [06:12<00:00, 18.65s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loop over batches\n",
    "with concurrent.futures.ProcessPoolExecutor(WORKERS) as executor:    \n",
    "    jobs = []\n",
    "    for source_path in BATCHES_DIR.glob(\"*.csv\"):\n",
    "        jobs.append(executor.submit(process_batch, source_path))\n",
    "    for job in tqdm(jobs):\n",
    "        job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e1a2ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_legacy_schema(dfr):\n",
    "    \"\"\"\n",
    "    Map data in the AI offline Parcel data schema to the legacy format\n",
    "    \"\"\"\n",
    "    # Dominant roof materials\n",
    "    dfr[\"dominant_roof_material\"] = \"unknown\"\n",
    "    dfr.loc[dfr[\"roof_present\"] == \"N\", \"dominant_roof_material\"] = \"not available\"\n",
    "    dfr[\"dominant_roof_material_confidence\"] = 1.0\n",
    "\n",
    "    for name, cname in [(\"Tile Roof\", \"tile\"), (\"Shingle Roof\", \"shingle\"), (\"Metal Roof\", \"metal\")]:\n",
    "        mask = dfr[f\"primary_roof_{cname}_roof_dominant\"] == \"Y\"\n",
    "        dfr.loc[mask, \"dominant_roof_material\"] = name\n",
    "        dfr.loc[mask, \"dominant_roof_material_confidence\"] = dfr.loc[mask, f\"primary_roof_{cname}_roof_confidence\"]\n",
    "        \n",
    "    # Num storeys\n",
    "    dfr[\"storey_category\"] = \"not available\"\n",
    "    dfr[\"storey_category_confidence\"] = 1.0\n",
    "\n",
    "    storey_categories = dfr[[c for c in dfr.columns if \"num_storeys\" in c]].idxmax(axis=1)\n",
    "    for storeys in [\"1\", \"2\", \"3+\"]:\n",
    "        mask = storey_categories == f\"primary_building_num_storeys_{storeys}_confidence\"\n",
    "        dfr.loc[mask, \"storey_category\"] = storeys\n",
    "        dfr.loc[mask, \"storey_category_confidence\"] = dfr.loc[mask, f\"primary_building_num_storeys_{storeys}_confidence\"]\n",
    "\n",
    "    # Direct column mappings\n",
    "    direct_mappings = {\n",
    "         'parcelPtId': 'parcel_id',\n",
    "         'geometry': 'wkt',\n",
    "         'link': 'mapbrowser_url',\n",
    "         'date': 'survey_date',\n",
    "         'primary_roof_tree_overhang_present': 'tree_overhang_present',\n",
    "         'primary_roof_tree_overhang_confidence': 'tree_overhang_confidence',\n",
    "         'primary_roof_hip_present': 'hip_roof_type_present',\n",
    "         'primary_roof_hip_confidence': 'hip_roof_type_confidence',\n",
    "         'primary_roof_gable_present': 'gable_roof_type_present',\n",
    "         'primary_roof_gable_confidence': 'gable_roof_type_confidence',\n",
    "         'primary_roof_flat_present': 'flat_roof_type_present',\n",
    "         'primary_roof_flat_confidence': 'flat_roof_type_confidence',\n",
    "         'primary_roof_turret_present': 'turret_roof_type_present',\n",
    "         'primary_roof_turret_confidence': 'turret_roof_type_confidence',\n",
    "         'primary_roof_pitch_degrees': 'roof_pitch_degrees',\n",
    "         'primary_building_area_sqft': 'area_under_roof_sqft',\n",
    "         'primary_building_area_sqm': 'area_under_roof_sqm',\n",
    "         'primary_building_height_m': 'building_height_m',\n",
    "         'primary_building_height_ft': 'building_height_ft'\n",
    "    }\n",
    "\n",
    "    dfr = dfr.rename(columns=direct_mappings)\n",
    "\n",
    "    # Map bools\n",
    "    for c in dfr.columns:\n",
    "        if \"_present\" in c:\n",
    "            dfr = dfr.replace({c: {\"Y\": \"True\", \"N\": \"False\"}})\n",
    "            dfr[c] = dfr[c].fillna(\"False\")\n",
    "        if \"_confidence\" in c:\n",
    "            dfr[c] = dfr[c].fillna(1.0)\n",
    "\n",
    "    # Filter columns\n",
    "    final_columns = [\n",
    "         'parcel_id',\n",
    "         'wkt',\n",
    "         'mapbrowser_url',\n",
    "         'survey_date',\n",
    "         'roof_present',\n",
    "         'roof_confidence',\n",
    "         'dominant_roof_material',\n",
    "         'dominant_roof_material_confidence',\n",
    "         'solar_panel_present',\n",
    "         'solar_panel_confidence',\n",
    "         'tree_overhang_present',\n",
    "         'tree_overhang_confidence',\n",
    "         'swimming_pool_present',\n",
    "         'swimming_pool_confidence',\n",
    "         'trampoline_present',\n",
    "         'trampoline_confidence',\n",
    "         'hip_roof_type_present',\n",
    "         'hip_roof_type_confidence',\n",
    "         'gable_roof_type_present',\n",
    "         'gable_roof_type_confidence',\n",
    "         'flat_roof_type_present',\n",
    "         'flat_roof_type_confidence',\n",
    "         'turret_roof_type_present',\n",
    "         'turret_roof_type_confidence',\n",
    "         'roof_pitch_degrees',\n",
    "         'area_under_roof_sqm',\n",
    "         'area_under_roof_sqft',\n",
    "         'building_height_m',\n",
    "         'building_height_ft',\n",
    "         'storey_category',\n",
    "         'storey_category_confidence'\n",
    "    ]\n",
    "\n",
    "    dfr = dfr[final_columns]\n",
    "    return dfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3baaf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine batches and map to legacy format\n",
    "for source in SOURCE_DIR.glob(\"*.csv\"):\n",
    "    dfs = []\n",
    "    for p in PROCESSED_DIR.glob(f\"*{source.stem}*.csv\"):\n",
    "        dfs.append(pd.read_csv(p))\n",
    "    full_df = pd.concat(dfs)\n",
    "    full_df.to_csv(OUTPUT_DIR / f\"{source.stem}.csv\", index=False)\n",
    "    transformed_full_df = map_to_legacy_schema(full_df)\n",
    "    transformed_full_df.to_csv(LEGACY_DIR / f\"{source.stem}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0eee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
